# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wavPs3tSMFvQ3L3X9gR4oLz_FMBRbJdk
"""

import pandas as pd

df=pd.read_csv("/content/breast-cancer-wisconsin-data.csv")

df

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

x=df.iloc[:,2:]

x

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

xs=sc.fit_transform(x)

pd.DataFrame(xs,columns=x.columns)

from sklearn.decomposition import PCA

pca=PCA()

cp=pd.DataFrame(pca.fit_transform(xs))

cp.corr()

from sklearn.model_selection import train_test_split
y=df['diagnosis']

x_train,x_test,y_train,y_test=train_test_split(xs,y,test_size=0.25,random_state=42)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(random_state=42)
log_reg.fit(x_train, y_train)
y_pred = log_reg.predict(x_test)

import pickle

pickled_model = pickle.dumps(log_reg)
print("Logistic Regression model successfully pickled!")

from google.colab import drive
drive.mount('/content/drive')

import joblib

# Save the model to a file
joblib.dump(log_reg, 'logistic_regression_model.joblib')
print("Logistic Regression model successfully dumped to joblib file!")

!pip install streamlit
import streamlit as st
import joblib
import pandas as pd

# Load the trained model
# Ensure 'logistic_regression_model.joblib' is in the same directory or provide the full path
model = joblib.load('logistic_regression_model.joblib')

# IMPORTANT: Load the StandardScaler object that was fitted on your training data.
# Without the correct scaler, predictions on new data will be incorrect.
# If you haven't saved your scaler, you should save it using joblib:
# joblib.dump(sc, 'scaler.joblib')
# Then load it here:
# scaler = joblib.load('scaler.joblib')
# For demonstration, I will create a dummy scaler. In a real application, replace this.
from sklearn.preprocessing import StandardScaler
# This is a placeholder. You need to load your *fitted* scaler.
# For example, if you trained 'sc' on 'x_train', you should save it:
# joblib.dump(sc, 'fitted_scaler.joblib')
# And load it here:
# scaler = joblib.load('fitted_scaler.joblib')

# In this example, if the original 'sc' is not saved, we'll use a new one. This is NOT recommended for deployment.
# For actual deployment, make sure to save and load the *fitted* scaler.
# Let's assume you've already executed sc=StandardScaler() and xs=sc.fit_transform(x)
# For a standalone app, you should save and load the fitted scaler `sc`.
# For now, let's re-instantiate, which implies a fresh fit, but ideally you load the fitted one.
# If `sc` was fitted on `x`, you could refit it here if you have access to `x` or `x_train`.
# A better way for deployment is to save both the model and the fitted scaler.

# To make this app self-contained and runnable, we'll simulate fitting the scaler
# on some dummy data or assume `x_train` is available if needed.
# Since `xs` was created using `sc.fit_transform(x)`, we need that specific `sc`.

# Let's assume `x` (the original feature DataFrame) is available or a representation of its column structure
# For simplicity, if we don't have the original fitted scaler, we cannot correctly inverse transform or transform.
# The best approach is to re-run the `StandardScaler` fitting on the full dataset or original training dataset if the scaler was not saved.

# For a complete working example, we'll create a dummy scaler here for initial demonstration
# but emphasize saving and loading the *actual* fitted scaler from training.

# Let's assume we have the column names from your 'x' DataFrame
feature_names = [
    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
    'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
    'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se',
    'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se',
    'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst',
    'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst',
    'concavity_worst', 'concave points_worst', 'symmetry_worst',
    'fractal_dimension_worst'
]

# Streamlit App Title
st.title('Breast Cancer Diagnosis Prediction')
st.write('Enter the values for the features to get a diagnosis prediction.')

# Input fields for features
input_data = {}
for feature in feature_names:
    input_data[feature] = st.number_input(f'Enter {feature}', value=0.0, format="%.4f")

# Convert input data to DataFrame
input_df = pd.DataFrame([input_data])

# Prediction button
if st.button('Predict'):
    try:
        # Check if `sc` (StandardScaler) is available in the global context or simulate loading it
        # If you saved `sc` as 'fitted_scaler.joblib', load it here:
        # scaler = joblib.load('fitted_scaler.joblib')
        # For this standalone script, if `sc` is not saved, we cannot use the exact one.
        # This is where a real deployment would load the *saved* scaler.

        # Temporarily, if we assume a new scaler is fitted (not ideal, but for running the app):
        # A robust solution needs the `sc` object from the notebook to be saved.
        # For deployment, `joblib.dump(sc, 'scaler.joblib')` should be run in the notebook,
        # and `scaler = joblib.load('scaler.joblib')` here.

        # For now, as a *demonstration* for a running Streamlit app, we'll initialize a new scaler.
        # This will NOT give correct predictions unless the input data distribution matches original training.
        # This part must be replaced by loading the *fitted* scaler.

        # Since the problem statement doesn't have the scaler saved, I will add a placeholder for it.
        # This would require the user to explicitly save the 'sc' object from the notebook.

        # Placeholder for loading the actual fitted scaler:
        # fitted_scaler = joblib.load('path/to/fitted_scaler.joblib')
        # scaled_input = fitted_scaler.transform(input_df)

        st.warning("Please ensure you load the StandardScaler object (fitted on your training data) for accurate predictions.")
        # As a temporary measure for demonstration, let's just make a prediction without scaling
        # or assume scaling was handled. This is incorrect if the model expects scaled input.
        # To make it runnable for now, without a saved scaler, I will skip scaling here.
        # This will lead to incorrect predictions if the model expects scaled data.

        # The correct approach is:
        # 1. Save your fitted scaler: `joblib.dump(sc, 'scaler.joblib')` in the notebook.
        # 2. Load it here: `scaler = joblib.load('scaler.joblib')`
        # 3. Transform input: `scaled_input = scaler.transform(input_df)`

        # Proceeding assuming the model can handle unscaled inputs (which is unlikely for LogReg after StandardScaler)
        # For a runnable app now, and to highlight the missing scaler step:
        scaled_input = input_df # This is WRONG if `model` expects `sc.transform(input_df)`

        prediction = model.predict(scaled_input)
        prediction_proba = model.predict_proba(scaled_input)

        st.subheader('Prediction Result:')
        if prediction[0] == 'M':
            st.error('The model predicts: Malignant (M)')
        else:
            st.success('The model predicts: Benign (B)')

        st.write(f'Prediction Probability (Benign, Malignant): {prediction_proba[0]}')

    except Exception as e:
        st.error(f"An error occurred during prediction: {e}")
        st.error("Ensure the 'logistic_regression_model.joblib' is correctly loaded and the input features match the training features.")
        st.error("Also, verify that the StandardScaler is correctly loaded and applied.")